# Quickstart

This notebook will:

1. Load the datasets: `data/train.pkl`, `data/x_test.pkl` and `data/y_test_local.pkl`.
2. Fit per-window ARIMA models on 60-minute input sequences to forecast the following 10 minutes.
3. Perform evaluation:
   - **Local validation** on `y_test_local.pkl` (window_id 1â€“2)  
   - **Official metrics:** MSE, MAE, IC, IR, Sharpe Ratio, MDD, VaR, ES  
   - **Trading snapshots:** CSM and LOTQ
4. Run inference on `x_test` and generate a PICKLE submission file at `sample_submission/submission.pkl`.
5. Save dummy `model_weights.pkl` into `sample_submission/` for submission compatibility. *(note: this is only a minimal example; in real deep learning models the weights file would be generated automatically during training.)*
import os, sys, json, time, warnings
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
# Paths (adjust if your layout differs)
ROOT = Path.cwd().parent if (Path.cwd().name == 'src') else Path.cwd()
DATA = ROOT / "data"
SRC  = ROOT / "src"
SUBM = ROOT / "sample_submission"

# Ensure src is importable
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))

# Create sample_submission dir if missing
SUBM.mkdir(parents=True, exist_ok=True)

SEED = 1337
np.random.seed(SEED)
torch.manual_seed(SEED)

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
DEVICE
# Load dataset files
info_path = DATA / "dataset_info.json"
if info_path.exists():
    info = json.loads(info_path.read_text(encoding="utf-8"))
    print("dataset_info.json loaded. Keys:", list(info.keys()))
    print(json.dumps({k: info[k] for k in ['features','input_len','horizon_len','outputs']}, indent=2))
else:
    print("dataset_info.json not found at", info_path)

# Peek train / x_test
train_path = DATA / "train.pkl"
x_test_path  = DATA / "x_test.pkl"
y_local_path = DATA / "y_test_local_time.pkl"

train = pd.read_pickle(train_path)
x_test  = pd.read_pickle(x_test_path)
y_test_local = pd.read_pickle(y_local_path)

print("train shape:", train.shape, "| columns:", train.columns.tolist())
print("x_test  shape:", x_test.shape,  "| columns:", x_test.columns.tolist())
print("y_test_local_time shape:", y_test_local.shape, "| columns:", y_test_local.columns.tolist())

display(train.head(3))
display(x_test.head(3))
display(y_test_local.head(3))
# Use the sampler logic from src/dataset.py to slice windows
from dataset import TrainWindowSampler

class WindowsDataset(Dataset):
    """
    Wrap TrainWindowSampler into a PyTorch Dataset.
    Returns:
      X: (60, 2) float32 -> [close, volume]
      y: (10,)  float32 -> future close
    """
    def __init__(self, train_path: str, rolling: bool = True, step_size: int = 1, max_samples: int = None):
        self.sampler = TrainWindowSampler(
            train_path=train_path,
            window=70,
            input_len=60,
            horizon_len=10,
            rolling=rolling,
            step_size=step_size,
            seed=SEED,
        )
        # Materialize (optionally capped) for stable batching
        xs, ys = [], []
        for i, (X, y) in enumerate(self.sampler.iter_windows()):
            xs.append(X.astype(np.float32))
            ys.append(y.astype(np.float32))
            if max_samples is not None and (i + 1) >= max_samples:
                break
        self.X = np.stack(xs, axis=0) if xs else np.zeros((0,60,2), dtype=np.float32)
        self.y = np.stack(ys, axis=0) if ys else np.zeros((0,10), dtype=np.float32)

    def __len__(self):  return len(self.X)
    def __getitem__(self, i):
        return torch.from_numpy(self.X[i]), torch.from_numpy(self.y[i])

# For a quick demo, cap samples. Increase for better quality.
MAX_SAMPLES = 50000  # set to None to use all windows
train_ds = WindowsDataset(str(train_path), rolling=True, step_size=1, max_samples=MAX_SAMPLES)
len(train_ds), train_ds.X.shape, train_ds.y.shape
# ARIMA baseline
from baselines.arima import ARIMABaseline

ari = ARIMABaseline(order=(1,1,0), maxiter=50)
ari.fit(train)

# Save dummy weights after fitting
import pickle
from pathlib import Path

weights_out = SUBM / "model_weights.pkl"
with open(weights_out, "wb") as f:
    pickle.dump({"config": ari.cfg.__dict__}, f)

print("Saved dummy weights to", weights_out)
# ARIMAX baseline (use volume as exogenous)
from baselines.arimax import ARIMAXBaseline

ari = ARIMAXBaseline(order=(1,1,0), maxiter=50)
ari.fit(train)
# Fast preview inference on a subset of x_test (NOT for official submission).
# For official submission, run full inference over all windows.

FIRST_N_WINDOWS = 500       # set to an integer (e.g., 500). Set to None to disable.

all_wids = x_test['window_id'].drop_duplicates().astype('int32').to_numpy()
base_sel = all_wids[:int(FIRST_N_WINDOWS)] if FIRST_N_WINDOWS is not None else all_wids # you need to run on all windows for official submission

must_wids = np.array([1, 2], dtype=np.int32)
exist_mask = np.isin(must_wids, all_wids)
if not exist_mask.all():
    missing = must_wids[~exist_mask].tolist()
    warnings.warn(f"[Preview] Required window_id(s) not in x_test: {missing}")
sel_wids = np.unique(np.concatenate([base_sel, must_wids[exist_mask]]))
print(f"Infer on {len(sel_wids)} / {len(all_wids)} windows "
      f"(forced include: {must_wids[exist_mask].tolist()})")

# Build a subset view (optional when running preview)
x_test_view = x_test[x_test['window_id'].isin(sel_wids)] if FIRST_N_WINDOWS is not None else x_test

# predict -> submission-like DataFrame
submission_df = ari.predict_x_test(x_test_view)   # columns: window_id, time_step, pred_close

# validate shape for selected windows
if not submission_df.empty:
    counts = submission_df.groupby('window_id')['time_step'].nunique()
    assert (counts == 10).all(), "Each selected window_id must have exactly 10 rows (0..9)."

# Save preview (NOT for official submission)
# For official submission, run inference on ALL windows and save to sample_submission/submission.pkl
# out_path = SUBM / "submission.pkl"
out_path = SUBM / "submission_example.pkl"
submission_df.to_pickle(out_path)
print(f"Saved preview to {out_path}  rows={len(submission_df)}  "
      f"windows={submission_df['window_id'].nunique()}")
display(submission_df.head(12))

print("NOTE: This is a PREVIEW subset. For official submission, you must run full inference on ALL windows.")
# Local test on window_id {1,2} with y_test_local.pkl
if not y_local_path.exists():
    warnings.warn(f"y_test_local_time.pkl not found at: {y_local_path}. Skip local eval.")
else:
    # NOTE: updated evaluate_all_metrics expects (y_true, y_pred, x_like, y_true_with_base, horizon_step)
    from metrics import evaluate_all_metrics

    target_wids = [1, 2]
    y_local = pd.read_pickle(y_local_path)     # ground truth: ['window_id','time_step','close','event_datetime','token']
    pred_local = submission_df[submission_df["window_id"].isin(target_wids)].copy()

    # Integrity check: each selected window must have exactly 10 prediction steps
    if not pred_local.empty:
        _c = pred_local.groupby("window_id")["time_step"].nunique()
        assert (_c == 10).all(), f"Incomplete prediction steps: {_c.to_dict()}"

    # Build x_like from x_test: use time_step == 59 as base_close reference
    x_test_local = x_test[x_test["window_id"].isin(target_wids)].copy()

    # Normalize dtypes for consistency
    for df in (y_local, pred_local, x_test_local):
        if "window_id" in df: df["window_id"] = df["window_id"].astype("int32")
        if "time_step" in df: df["time_step"] = df["time_step"].astype("int8")
        if "close" in df: df["close"] = df["close"].astype("float32")
        if "pred_close" in df: df["pred_close"] = df["pred_close"].astype("float32")

    # Keep only ground truth for {1,2}
    y_true_local = y_local[y_local["window_id"].isin(target_wids)].copy()

    # Verify that y_true_local has event_datetime column
    if "event_datetime" not in y_true_local.columns:
        raise ValueError("y_test_local_time.pkl must contain 'event_datetime' column")
    
    print(f"\ny_true_local columns: {y_true_local.columns.tolist()}")
    print(f"pred_local columns: {pred_local.columns.tolist()}")
    print(f"x_test_local columns: {x_test_local.columns.tolist()}")

    # Compute metrics: error metrics + strategy-based (CSM/LOTQ/PW) Sharpe, MDD, VaR, ES
    try:
        metrics_dict = evaluate_all_metrics(
            y_true=y_true_local,
            y_pred=pred_local,
            x_test=x_test_local,
            alpha=0.05,
        )
        
        print("\n" + "="*60)
        print("Local Evaluation on window_id 1 & 2 (using NEW metrics)")
        print("="*60)
        
        # Display metrics in a nice format
        metrics_df = pd.DataFrame([metrics_dict]).T.rename(columns={0: "value"})
        display(metrics_df)
    
        
    except Exception as e:
        print(f"\nâŒ Error during metric evaluation: {e}")
        import traceback
        traceback.print_exc()

print("\n" + "="*60)
print("Script completed successfully!")
print("="*60)
# Test different ARIMA orders to find the one with minimum MSE
from itertools import product
import time

# Define parameter ranges to test
p_range = [0, 1, 2]  # AR order
d_range = [0, 1]     # Differencing order
q_range = [0, 1, 2]  # MA order

# Generate all combinations
orders_to_test = list(product(p_range, d_range, q_range))
# Remove (0,0,0) as it's not a valid ARIMA model
orders_to_test = [order for order in orders_to_test if not all(x == 0 for x in order)]

print(f"Testing {len(orders_to_test)} different ARIMA orders...")
print("Orders to test:", orders_to_test)

results = []

for i, order in enumerate(orders_to_test):
    print(f"\n[{i+1}/{len(orders_to_test)}] Testing ARIMA{order}...")
    
    try:
        start_time = time.time()
        
        # Create and fit ARIMA model
        ari_test = ARIMABaseline(order=order, maxiter=50)
        ari_test.fit(train)
        
        # Run inference on test windows (using same subset as before)
        submission_test = ari_test.predict_x_test(x_test_view)
        
        # Calculate metrics on local validation set
        pred_test = submission_test[submission_test["window_id"].isin(target_wids)].copy()
        
        if not pred_test.empty:
            # Normalize dtypes
            for df in (pred_test,):
                if "window_id" in df: df["window_id"] = df["window_id"].astype("int32")
                if "time_step" in df: df["time_step"] = df["time_step"].astype("int8")
                if "pred_close" in df: df["pred_close"] = df["pred_close"].astype("float32")
            
            # Calculate metrics
            metrics_test = evaluate_all_metrics(
                y_true=y_true_local,
                y_pred=pred_test,
                x_test=x_test_local,
                alpha=0.05,
            )
            
            elapsed_time = time.time() - start_time
            
            result = {
                'order': order,
                'p': order[0],
                'd': order[1], 
                'q': order[2],
                'MSE': metrics_test['MSE'],
                'MAE': metrics_test['MAE'],
                'IC': metrics_test['IC'],
                'IR': metrics_test['IR'],
                'SharpeRatio': metrics_test['SharpeRatio'],
                'MDD': metrics_test['MDD'],
                'VaR': metrics_test['VaR'],
                'ES': metrics_test['ES'],
                'time_seconds': elapsed_time
            }
            
            results.append(result)
            print(f"  MSE: {metrics_test['MSE']:.2e}, MAE: {metrics_test['MAE']:.2e}, Time: {elapsed_time:.1f}s")
        else:
            print(f"  No predictions generated for order {order}")
            
    except Exception as e:
        print(f"  Error with order {order}: {str(e)}")
        continue

print(f"\n\nCompleted testing {len(results)} models successfully!")
# Analyze results and find best performing orders
if results:
    results_df = pd.DataFrame(results)
    
    # Sort by MSE (ascending) to find best models
    results_df_sorted = results_df.sort_values('MSE')
    
    print("=" * 80)
    print("ARIMA ORDER COMPARISON RESULTS")
    print("=" * 80)
    
    # Display top 5 models by MSE
    print("\nTOP 5 MODELS (sorted by MSE):")
    print("-" * 50)
    top_5 = results_df_sorted.head(5)
    for idx, row in top_5.iterrows():
        print(f"ARIMA{row['order']} - MSE: {row['MSE']:.2e}, MAE: {row['MAE']:.2e}, Sharpe: {row['SharpeRatio']:.3f}")
    
    print("\nDETAILED RESULTS TABLE:")
    print("-" * 50)
    display_cols = ['p', 'd', 'q', 'MSE', 'MAE', 'IC', 'SharpeRatio', 'time_seconds']
    display(results_df_sorted[display_cols].round(6))
    
    # Find best model
    best_order = results_df_sorted.iloc[0]['order']
    best_mse = results_df_sorted.iloc[0]['MSE']
    
    print(f"\nğŸ† BEST MODEL: ARIMA{best_order}")
    print(f"   MSE: {best_mse:.2e}")
    print(f"   Improvement over ARIMA(1,1,0): {((metrics_dict['MSE'] - best_mse) / metrics_dict['MSE'] * 100):.2f}%")
    
    # Compare different aspects
    print("\nCOMPARISON BY DIFFERENT METRICS:")
    print("-" * 40)
    print(f"Best MSE: ARIMA{results_df_sorted.iloc[0]['order']} ({results_df_sorted.iloc[0]['MSE']:.2e})")
    print(f"Best MAE: ARIMA{results_df_sorted.sort_values('MAE').iloc[0]['order']} ({results_df_sorted.sort_values('MAE').iloc[0]['MAE']:.2e})")
    print(f"Best Sharpe: ARIMA{results_df_sorted.sort_values('SharpeRatio', ascending=False).iloc[0]['order']} ({results_df_sorted.sort_values('SharpeRatio', ascending=False).iloc[0]['SharpeRatio']:.3f})")
    print(f"Best IC: ARIMA{results_df_sorted.sort_values('IC', ascending=False).iloc[0]['order']} ({results_df_sorted.sort_values('IC', ascending=False).iloc[0]['IC']:.3f})")
else:
    print("No successful results to analyze.")
# Optional: Create visualization of results
if results and len(results) > 1:
    import matplotlib.pyplot as plt
    
    # Take only top 10 models for cleaner visualization
    top_10 = min(10, len(results))
    results_df_top10 = results_df.sort_values('MSE').head(top_10)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'ARIMA Order Performance Comparison (Top {top_10} Models)', fontsize=16)
    
    # Plot MSE (sorted by MSE)
    axes[0,0].bar(range(len(results_df_top10)), results_df_top10['MSE'])
    axes[0,0].set_title('MSE by ARIMA Order')
    axes[0,0].set_ylabel('MSE')
    axes[0,0].set_xticks(range(len(results_df_top10)))
    axes[0,0].set_xticklabels([str(order) for order in results_df_top10['order']], rotation=45)
    axes[0,0].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    
    # Plot MAE (using same top 10 by MSE for consistency)
    axes[0,1].bar(range(len(results_df_top10)), results_df_top10['MAE'])
    axes[0,1].set_title('MAE by ARIMA Order')
    axes[0,1].set_ylabel('MAE')
    axes[0,1].set_xticks(range(len(results_df_top10)))
    axes[0,1].set_xticklabels([str(order) for order in results_df_top10['order']], rotation=45)
    
    # Plot Sharpe Ratio (top 10 by Sharpe)
    sharpe_top10 = results_df.sort_values('SharpeRatio', ascending=False).head(top_10)
    axes[1,0].bar(range(len(sharpe_top10)), sharpe_top10['SharpeRatio'])
    axes[1,0].set_title('Sharpe Ratio by ARIMA Order')
    axes[1,0].set_ylabel('Sharpe Ratio')
    axes[1,0].set_xticks(range(len(sharpe_top10)))
    axes[1,0].set_xticklabels([str(order) for order in sharpe_top10['order']], rotation=45)
    
    # Plot IC (top 10 by IC)
    ic_top10 = results_df.sort_values('IC', ascending=False).head(top_10)
    axes[1,1].bar(range(len(ic_top10)), ic_top10['IC'])
    axes[1,1].set_title('Information Coefficient by ARIMA Order')
    axes[1,1].set_ylabel('IC')
    axes[1,1].set_xticks(range(len(ic_top10)))
    axes[1,1].set_xticklabels([str(order) for order in ic_top10['order']], rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nğŸ“Š Visualization completed. Showing top {top_10} models for each metric.")
# ==========================
# Save best weights & full submission
# ==========================
from pathlib import Path
import pickle

# 1) ä»¥æœ€ä½³ order é‡æ–°å»ºæ¨¡ + è¨“ç·´
BEST_MAXITER = 150  # ä½ å¯èª¿æ•´
ari_best = ARIMAXBaseline(order=(0, 1, 1), maxiter=BEST_MAXITER)
ari_best.fit(train)

# 2) ç”¨ã€Œå…¨é‡ã€x_test ç”¢ç”Ÿæ­£å¼ submissionï¼ˆä¸è¦ç”¨å…ˆå‰çš„ subsetï¼‰
submission_full = ari_best.predict_x_test(x_test.copy())

# 3) åŸºæœ¬å®Œæ•´æ€§æª¢æŸ¥ï¼šæ¯å€‹ window å¿…é ˆå‰›å¥½ 10 åˆ— time_step=0..9
if not submission_full.empty:
    counts = submission_full.groupby('window_id')['time_step'].nunique()
    assert (counts == 10).all(), "Each window_id must have exactly 10 prediction steps (0..9)."

# 4) å„²å­˜æ¬Šé‡ï¼ˆconfigï¼‰åˆ° sample_submission/model_weights.pkl
SUBM.mkdir(parents=True, exist_ok=True)
weights_path = SUBM / "model_weights.pkl"
with open(weights_path, "wb") as f:
    pickle.dump(
        {
            "config": {
                "order": tuple(map(int, best_order)),
                "seasonal_order": (0, 0, 0, 0),
                "enforce_stationarity": False,
                "enforce_invertibility": False,
                "trend": None,
                "maxiter": int(BEST_MAXITER),
                "tol": 1e-6,
                "disp": False,
                "fallback": "naive_last",
                "seed": 1337,
            }
        },
        f,
    )
print(f"âœ… Saved best config to: {weights_path}")

# 5) è¼¸å‡ºæ­£å¼æäº¤æª”ï¼ˆè©•åˆ†ç³»çµ±é æœŸæª”åé€šå¸¸æ˜¯ submission.pklï¼‰
submission_path = SUBM / "submission.pkl"          # â† æ­£å¼ä¸Šå‚³ç”¨
# submission_path = SUBM / "submission_example.pkl" # â† è‹¥ä½ æƒ³ä¿ç•™æˆ example
submission_full.to_pickle(submission_path)
print(f"âœ… Saved FULL submission to: {submission_path} | rows={len(submission_full)} | windows={submission_full['window_id'].nunique()}")

# 6) é¡¯ç¤ºå‰å¹¾åˆ—ç¢ºèª
display(submission_full.head(12))
# Local test on window_id {1,2} with y_test_local.pkl

submission_df = submission_full.copy()

print("Using order=", best_order)

if not y_local_path.exists():
    warnings.warn(f"y_test_local_time.pkl not found at: {y_local_path}. Skip local eval.")
else:
    # NOTE: updated evaluate_all_metrics expects (y_true, y_pred, x_like, y_true_with_base, horizon_step)
    from metrics import evaluate_all_metrics

    target_wids = [1, 2]
    y_local = pd.read_pickle(y_local_path)     # ground truth: ['window_id','time_step','close','event_datetime','token']
    pred_local = submission_df[submission_df["window_id"].isin(target_wids)].copy()

    # Integrity check: each selected window must have exactly 10 prediction steps
    if not pred_local.empty:
        _c = pred_local.groupby("window_id")["time_step"].nunique()
        assert (_c == 10).all(), f"Incomplete prediction steps: {_c.to_dict()}"

    # Build x_like from x_test: use time_step == 59 as base_close reference
    x_test_local = x_test[x_test["window_id"].isin(target_wids)].copy()

    # Normalize dtypes for consistency
    for df in (y_local, pred_local, x_test_local):
        if "window_id" in df: df["window_id"] = df["window_id"].astype("int32")
        if "time_step" in df: df["time_step"] = df["time_step"].astype("int8")
        if "close" in df: df["close"] = df["close"].astype("float32")
        if "pred_close" in df: df["pred_close"] = df["pred_close"].astype("float32")

    # Keep only ground truth for {1,2}
    y_true_local = y_local[y_local["window_id"].isin(target_wids)].copy()

    # Verify that y_true_local has event_datetime column
    if "event_datetime" not in y_true_local.columns:
        raise ValueError("y_test_local_time.pkl must contain 'event_datetime' column")
    
    print(f"\ny_true_local columns: {y_true_local.columns.tolist()}")
    print(f"pred_local columns: {pred_local.columns.tolist()}")
    print(f"x_test_local columns: {x_test_local.columns.tolist()}")

    # Compute metrics: error metrics + strategy-based (CSM/LOTQ/PW) Sharpe, MDD, VaR, ES
    try:
        metrics_dict = evaluate_all_metrics(
            y_true=y_true_local,
            y_pred=pred_local,
            x_test=x_test_local,
            alpha=0.05,
        )
        
        print("\n" + "="*60)
        print("Local Evaluation on window_id 1 & 2 (using NEW metrics)")
        print("="*60)
        
        # Display metrics in a nice format
        metrics_df = pd.DataFrame([metrics_dict]).T.rename(columns={0: "value"})
        display(metrics_df)
    
        
    except Exception as e:
        print(f"\nâŒ Error during metric evaluation: {e}")
        import traceback
        traceback.print_exc()

print("\n" + "="*60)
print("Script completed successfully!")
print("="*60)
# ============================================================
# Split train.pkl into 80% training and 20% validation sets
# ============================================================

import pandas as pd
from pathlib import Path

train_path = DATA / "train.pkl"
train = pd.read_pickle(train_path)

print(f"Original train shape: {train.shape}")
print(f"Columns: {train.columns.tolist()}")

# --- Chronological split by series_id ---
train_80_list, val_20_list = [], []

for sid, g in train.groupby("series_id"):
    g = g.sort_values("time_step")  # keep chronological order
    n = len(g)
    split_idx = int(n * 0.8)
    train_80_list.append(g.iloc[:split_idx])
    val_20_list.append(g.iloc[split_idx:])

train_80 = pd.concat(train_80_list, ignore_index=True)
val_20   = pd.concat(val_20_list, ignore_index=True)

# --- Save the splits ---
train_80_path = DATA / "train_80.pkl"
val_20_path   = DATA / "val_20.pkl"

train_80.to_pickle(train_80_path)
val_20.to_pickle(val_20_path)

print(f"âœ… Saved train_80.pkl ({train_80.shape[0]} rows) â†’ {train_80_path}")
print(f"âœ… Saved val_20.pkl   ({val_20.shape[0]} rows) â†’ {val_20_path}")

# --- Quick verification ---
for sid, g in train.groupby("series_id"):
    n_total = len(g)
    n_train = len(train_80[train_80.series_id == sid])
    n_val   = len(val_20[val_20.series_id == sid])
    print(f"Series {sid}: total={n_total}, train={n_train}, val={n_val}")

# # %% 
# Cell A â€” è®€å– train_80 / val_20ï¼ˆå‰é¢å·²æœ‰åˆ‡åˆ†èˆ‡å­˜æª”ï¼‰
from pathlib import Path
import numpy as np
import pandas as pd

LOOKBACK = 60
HORIZON  = 10

train80_path = DATA / "train_80.pkl"
val20_path   = DATA / "val_20.pkl"

assert train80_path.exists(), f"train_80.pkl not found at {train80_path}"
assert val20_path.exists(),   f"val_20.pkl not found at {val20_path}"

train_80 = pd.read_pickle(train80_path)
val_20   = pd.read_pickle(val20_path)

# æ’åºï¼Œç¢ºä¿æ¯å€‹ series æ™‚é–“åºä¸€è‡´
val_20   = val_20.sort_values(["series_id","time_step"]).reset_index(drop=True)
train_80 = train_80.sort_values(["series_id","time_step"]).reset_index(drop=True)

print("train_80:", train_80.shape, "val_20:", val_20.shape)
print("columns:", train_80.columns.tolist())

# %% 
# Cell B (FAST, correct schema) â€” å¾ val_20 å»ºå‡ºï¼š
#   - x_val_likeï¼šæ¯å€‹ window_id æœ‰ 60 åˆ— context (time_step=0..59)ï¼ŒåŒ…å« ['series_id','time_step','close','volume']
#   - y_true_valï¼šæ¯å€‹ window_id æœ‰ 10 åˆ—æœªä¾†çœŸå€¼ (time_step=0..9)ï¼ŒåŒ…å« ['window_id','time_step','close']
from tqdm import tqdm
import numpy as np
import pandas as pd

LOOKBACK = 60
HORIZON  = 10

def build_val_context_and_targets(df, lookback=60, horizon=10,
                                  id_col="series_id", t_col="time_step",
                                  y_col="close", vol_col="volume"):
    df = df.sort_values([id_col, t_col]).reset_index(drop=True)

    xs, ys = [], []
    wid_base = 0
    n_series = df[id_col].nunique()

    for sid, g in tqdm(df.groupby(id_col), total=n_series, desc="Building 60->10 val windows"):
        g = g.reset_index(drop=True)
        n = len(g)
        num_win = n - lookback - horizon + 1
        if num_win <= 0:
            continue

        # context index: å°æ¯å€‹ window å– [end-lookback : end) é•·åº¦=60
        end_idx = np.arange(lookback, lookback + num_win, dtype=np.int64)   # shape (num_win,)
        ctx_offsets = np.arange(-lookback, 0, dtype=np.int64)               # [-60..-1]
        ctx_idx = (end_idx[:, None] + ctx_offsets[None, :])                 # shape (num_win, 60)

        # future index: [end : end+horizon) é•·åº¦=10
        fut_offsets = np.arange(horizon, dtype=np.int64)                    # [0..9]
        fut_idx = end_idx[:, None] + fut_offsets[None, :]                   # shape (num_win, 10)

        # ==== x_val_likeï¼šå±•å¹³æˆ DataFrame ====
        # window_id repeated for each of the 60 context steps
        window_ids_ctx = wid_base + np.repeat(np.arange(num_win, dtype=np.int32), lookback)
        # time_step = 0..59 for each window
        time_steps_ctx = np.tile(np.arange(lookback, dtype=np.int8), num_win)
        # series_id
        series_ids_ctx = np.full(num_win * lookback, sid, dtype=g[id_col].dtype)

        # å– context çš„ close/volume å€¼
        close_vals = g[y_col].to_numpy()
        vol_vals   = g[vol_col].to_numpy() if vol_col in g.columns else np.zeros_like(close_vals)

        ctx_close = close_vals[ctx_idx].reshape(-1).astype(np.float32)
        ctx_vol   = vol_vals[ctx_idx].reshape(-1).astype(np.float32)

        x_part = pd.DataFrame({
            "window_id": window_ids_ctx,
            "time_step": time_steps_ctx,   # 0..59
            "series_id": series_ids_ctx,
            "close": ctx_close,
            "volume": ctx_vol,
        })

        # ==== y_true_valï¼šå±•å¹³æˆ DataFrame ====
        window_ids_fut = wid_base + np.repeat(np.arange(num_win, dtype=np.int32), horizon)
        time_steps_fut = np.tile(np.arange(horizon, dtype=np.int8), num_win)
        fut_close = close_vals[fut_idx].reshape(-1).astype(np.float32)

        y_part = pd.DataFrame({
            "window_id": window_ids_fut,
            "time_step": time_steps_fut,   # 0..9
            "close": fut_close,
        })

        xs.append(x_part)
        ys.append(y_part)
        wid_base += num_win

        tqdm.write(f"[series {sid}] +{num_win} windows (cum windows: {wid_base})")

    x_val_like = pd.concat(xs, ignore_index=True) if xs else pd.DataFrame(columns=["window_id","time_step","series_id","close","volume"])
    y_true_val = pd.concat(ys, ignore_index=True) if ys else pd.DataFrame(columns=["window_id","time_step","close"])

    # dtypes
    for df_ in (x_val_like, y_true_val):
        if "window_id" in df_: df_["window_id"] = df_["window_id"].astype("int32")
        if "time_step" in df_: df_["time_step"] = df_["time_step"].astype("int8")
    if "close" in x_val_like: x_val_like["close"] = x_val_like["close"].astype("float32")
    if "volume" in x_val_like: x_val_like["volume"] = x_val_like["volume"].astype("float32")
    if "close" in y_true_val: y_true_val["close"] = y_true_val["close"].astype("float32")

    return x_val_like, y_true_val

x_val_view, y_true_val = build_val_context_and_targets(
    val_20, lookback=LOOKBACK, horizon=HORIZON,
    id_col="series_id", t_col="time_step", y_col="close", vol_col="volume"
)
print("x_val_view:", x_val_view.shape, "y_true_val:", y_true_val.shape)
print("#val windows:", x_val_view["window_id"].nunique())
print("x_val_view columns:", x_val_view.columns.tolist())   # æ‡‰åŒ…å« ['series_id','time_step','close','volume']

# %%
# Cell C â€” è©•åˆ†è¼”åŠ©ï¼šå„ªå…ˆç”¨ evaluate_all_metricsï¼›è‹¥ç„¡æ³•ï¼ˆç¼ºæ¬„ä½ï¼‰å‰‡ fallback è¨ˆç®— MSE/MAE
import numpy as np

def robust_evaluate(y_true_df, pred_df, x_like_df):
    """
    y_true_df: columns -> window_id, time_step, close
    pred_df:   columns -> window_id, time_step, pred_close
    x_like_df: ä»»æ„å‚³å…¥ï¼Œevaluate_all_metrics è‹¥éœ€è¦æœƒä½¿ç”¨
    """
    # å…ˆå˜—è©¦å®˜æ–¹ metrics
    try:
        metrics = evaluate_all_metrics(
            y_true=y_true_df,
            y_pred=pred_df,
            x_test=x_like_df,
            alpha=0.05,
        )
        # ç¢ºä¿è‡³å°‘æœ‰ MSE/MAE æ¬„ä½
        if "MSE" in metrics and "MAE" in metrics:
            return metrics
    except Exception as e:
        print("[metrics] Fallback due to:", e)

    # Fallback: merge å¾Œæ‰‹ç®— MSE/MAEï¼Œå…¶é¤˜ç”¨ NaN
    merged = y_true_df.merge(
        pred_df[["window_id","time_step","pred_close"]],
        on=["window_id","time_step"],
        how="inner",
    )
    if len(merged) == 0:
        return {"MSE": np.nan, "MAE": np.nan, "IC": np.nan, "IR": np.nan,
                "SharpeRatio": np.nan, "MDD": np.nan, "VaR": np.nan, "ES": np.nan}

    err = merged["pred_close"] - merged["close"]
    mse = float(np.mean(err**2))
    mae = float(np.mean(np.abs(err)))
    return {"MSE": mse, "MAE": mae, "IC": np.nan, "IR": np.nan,
            "SharpeRatio": np.nan, "MDD": np.nan, "VaR": np.nan, "ES": np.nan}

# åœ¨è·‘ grid search å‰æŠ½æ¨£ä¸€éƒ¨åˆ† validation windows
N_WINDOWS = 2000  # æˆ– 500/1000ï¼Œå…ˆèª¿åƒç”¨
rng = 42

keep_wids = (x_val_view['window_id']
             .drop_duplicates()
             .sample(n=min(N_WINDOWS, x_val_view['window_id'].nunique()), random_state=rng))

x_val_sampled = x_val_view[x_val_view['window_id'].isin(keep_wids)].copy()
y_val_sampled = y_true_val[y_true_val['window_id'].isin(keep_wids)].copy()

# æ¯å€‹æŠ½æ¨£ window æ˜¯å¦éƒ½æœ‰ 60 åˆ— context
assert (x_val_sampled.groupby('window_id')['time_step'].nunique() >= 60).all()
# y æ˜¯å¦éƒ½æœ‰ 10 åˆ—
assert (y_val_sampled.groupby('window_id')['time_step'].nunique() == 10).all()
# %%
# =====================================================
# proxy_metrics â€” simplified local evaluation (stable)
# =====================================================
import numpy as np
import pandas as pd
from scipy.stats import spearmanr

def _safe_spearman(a, b):
    """é˜²æ­¢ all-constant æˆ–ç©ºé™£åˆ—å ±éŒ¯"""
    a = np.asarray(a)
    b = np.asarray(b)
    if a.size == 0 or b.size == 0:
        return np.nan
    if np.all(a == a[0]) or np.all(b == b[0]):
        return np.nan
    return spearmanr(a, b, nan_policy="omit").correlation

def proxy_metrics(y_true_df, y_pred_df, x_like_df=None, alpha=0.05):
    """
    è¼•é‡ç‰ˆ metricsï¼Œç”¨æ–¼ Grid Searchï¼š
      - MSE, MAE
      - IC (Spearman)
      - IR (IC / std(IC))
      - SharpeRatio (ç°¡åŒ–ç‰ˆ)
    å…¶é¤˜å¡« NaN é¿å… KeyErrorã€‚
    """
    # === å°é½Š window_id, time_step ===
    df = y_true_df.merge(
        y_pred_df[["window_id", "time_step", "pred_close"]],
        on=["window_id", "time_step"],
        how="inner"
    ).copy()

    if len(df) == 0:
        return {
            "MSE": np.nan, "MAE": np.nan, "IC": np.nan, "IR": np.nan, "SharpeRatio": np.nan,
            "MDD": np.nan, "VaR": np.nan, "ES": np.nan
        }

    # === Basic errors ===
    err = df["pred_close"] - df["close"]
    mse = float(np.mean(err**2))
    mae = float(np.mean(np.abs(err)))

    # === å»ºç«‹ pseudo returns ===
    df["true_ret"] = df.groupby("window_id")["close"].pct_change().shift(-1)
    df["pred_ret"] = df.groupby("window_id")["pred_close"].pct_change().shift(-1)
    df = df.dropna(subset=["true_ret", "pred_ret"]).reset_index(drop=True)

    # === IC & IR ===
    ics_by_t = (
        df.groupby("time_step", group_keys=False)[["true_ret","pred_ret"]]
          .apply(lambda g: _safe_spearman(g["true_ret"], g["pred_ret"]))
          .astype(float)
    )
    ic = float(np.nanmean(ics_by_t)) if ics_by_t.notna().any() else 0.0
    ir = float(ic / (np.nanstd(ics_by_t, ddof=0) + 1e-9)) if ics_by_t.notna().sum() > 1 else 0.0

    # === Sharpe (proxy) ===
    q = (
        df.groupby("time_step", group_keys=False)[["true_ret","pred_ret"]]
          .apply(lambda g: g.loc[g["pred_ret"].rank(pct=True) >= 0.8, "true_ret"].mean())
          .astype(float)
    )
    sharpe = float(np.nanmean(q) / (np.nanstd(q, ddof=0) + 1e-9)) if q.notna().any() else 0.0

    # === Return consistent structure ===
    return {
        "MSE": mse,
        "MAE": mae,
        "IC": ic,
        "IR": ir,
        "SharpeRatio": sharpe,
        "MDD": np.nan,   # ä¿ç•™æ¬„ä½é¿å… KeyError
        "VaR": np.nan,
        "ES": np.nan,
    }

# ğŸ”¹ quick smoke test
# (use small slice of val set to verify no crash)
tmp = proxy_metrics(y_true_val.head(100), 
                    y_true_val.head(100).rename(columns={"close":"pred_close"}), 
                    x_val_view.head(100))
print("proxy_metrics test:", tmp)

# Test different ARIMAX orders to find the one with minimum MSE
from itertools import product
import time

# Define parameter ranges to test
p_range = [0, 1, 2]  # AR order
d_range = [1]     # Differencing order
q_range = [0, 1, 2]  # MA order

# Generate all combinations
orders_to_test = list(product(p_range, d_range, q_range))
# Remove (0,0,0) as it's not a valid ARIMAX model
orders_to_test = [order for order in orders_to_test if not all(x == 0 for x in order)]

print(f"Testing {len(orders_to_test)} different ARIMAX orders...")
print("Orders to test:", orders_to_test)

results = []

for i, order in enumerate(orders_to_test):
    print(f"\n[{i+1}/{len(orders_to_test)}] Testing ARIMAX{order}...")
    
    try:
        start_time = time.time()
        
        # Create and fit ARIMAX model
        ari_test = ARIMAXBaseline(order=order, maxiter=50)
        ari_test.fit(train_80)
        
        # Run inference on test windows (using same subset as before)
        submission_test = ari_test.predict_x_test(x_val_sampled.copy())
        
        # Calculate metrics on local validation set
        pred_test = submission_test[submission_test["window_id"].isin(keep_wids)].copy()
        

        if not pred_test.empty:
            counts = submission_test.groupby('window_id')['time_step'].nunique(); 
            assert (counts == 10).all()
            # Normalize dtypes
            for df in (pred_test,):
                if "window_id" in df: df["window_id"] = df["window_id"].astype("int32")
                if "time_step" in df: df["time_step"] = df["time_step"].astype("int8")
                if "pred_close" in df: df["pred_close"] = df["pred_close"].astype("float32")
            
            # # Calculate metrics
            # metrics_test = evaluate_all_metrics(
            #     y_true=y_val_sampled,
            #     y_pred=pred_test,
            #     x_test=x_val_sampled,
            #     alpha=0.05,
            # )

            # Calculate proxy metricsï¼ˆæœ¬åœ°æ’åºç”¨ï¼‰
            metrics_test = proxy_metrics(
                y_true_df=y_val_sampled,
                y_pred_df=pred_test,
                x_like_df=x_val_sampled,
            )
            
            elapsed_time = time.time() - start_time
            
            result = {
                'order': order,
                'p': order[0],
                'd': order[1], 
                'q': order[2],
                'MSE': metrics_test['MSE'],
                'MAE': metrics_test['MAE'],
                'IC': metrics_test['IC'],
                'IR': metrics_test['IR'],
                'SharpeRatio': metrics_test['SharpeRatio'],
                'MDD': metrics_test['MDD'],
                'VaR': metrics_test['VaR'],
                'ES': metrics_test['ES'],
                'time_seconds': elapsed_time
            }
            
            results.append(result)
            print(f"  MSE: {metrics_test['MSE']:.2e}, MAE: {metrics_test['MAE']:.2e}, Time: {elapsed_time:.1f}s")
        else:
            print(f"  No predictions generated for order {order}")
            
    except Exception as e:
        print(f"  Error with order {order}: {str(e)}")
        continue

print(f"\n\nCompleted testing {len(results)} models successfully!")
# %%
# Cell E â€” æ’åºçµæœ & å–å¾—æœ€ä½³ order
if results:
    results_df = pd.DataFrame(results)
    eps = 1e-9
    results_df["composite_score"] = (
        0.5 * (1 / (results_df["MSE"] + eps)) +
        0.5 * (1 / (results_df["MAE"] + eps)) 
        # 0.10 * results_df["IC"] +
        # 0.10 * results_df["IR"] +
        # 0.20 * results_df["SharpeRatio"] 
        # 0.05 * (-results_df["MDD"]) +
        # 0.05 * (-results_df["VaR"]) +
        # 0.05 * (-results_df["ES"])
    )
    results_df_sorted = results_df.sort_values("composite_score", ascending=False)
    print("="*80)
    print("ARIMAX ORDER COMPARISON RESULTS")
    print("="*80)

    print("\nTOP 5 MODELS (sorted by composite score):")
    print("-"*50)
    top5 = results_df_sorted.head(5)
    for _, r in top5.iterrows():
        print(f"ARIMAX{r['order']} - MSE: {r['MSE']:.2e}, MAE: {r['MAE']:.2e}, Sharpe: {r['SharpeRatio']}")
    try:
        from IPython.display import display
        display_cols = ["p","d","q","MSE","MAE","IC", "IR", "SharpeRatio", "MDD", "VaR", "ES", "time_seconds"]
        display(results_df_sorted[display_cols].round(6))
    except Exception:
        print(results_df_sorted.round(6).to_string(index=False))

    best_order = tuple(map(int, results_df_sorted.iloc[0][["p","d","q"]].tolist()))
    best_mse   = float(results_df_sorted.iloc[0]["MSE"])
    print(f"\nğŸ† BEST MODEL: ARIMAX{best_order}\n   MSE: {best_mse:.2e}")
else:
    print("No successful results to analyze.")

# ==========================
# Save best weights & full submission
# ==========================
from pathlib import Path
import pickle

# 1) ä»¥æœ€ä½³ order é‡æ–°å»ºæ¨¡ + è¨“ç·´
BEST_MAXITER = 150  # ä½ å¯èª¿æ•´
ORDER_TO_USE = tuple(map(int, best_order))
print(f"Using order={ORDER_TO_USE}, maxiter={BEST_MAXITER}")
ari_best = ARIMABaseline(order=ORDER_TO_USE, maxiter=BEST_MAXITER)
ari_best.fit(train)

# 2) ç”¨ã€Œå…¨é‡ã€x_test ç”¢ç”Ÿæ­£å¼ submissionï¼ˆä¸è¦ç”¨å…ˆå‰çš„ subsetï¼‰
submission_full = ari_best.predict_x_test(x_test.copy())

# 3) åŸºæœ¬å®Œæ•´æ€§æª¢æŸ¥ï¼šæ¯å€‹ window å¿…é ˆå‰›å¥½ 10 åˆ— time_step=0..9
if not submission_full.empty:
    counts = submission_full.groupby('window_id')['time_step'].nunique()
    assert (counts == 10).all(), "Each window_id must have exactly 10 prediction steps (0..9)."

# 4) å„²å­˜æ¬Šé‡ï¼ˆconfigï¼‰åˆ° sample_submission/model_weights.pkl
SUBM.mkdir(parents=True, exist_ok=True)
weights_path = SUBM / "model_weights.pkl"
with open(weights_path, "wb") as f:
    pickle.dump(
        {
            "config": {
                "order": tuple(map(int, best_order)),
                "seasonal_order": (0, 0, 0, 0),
                "enforce_stationarity": False,
                "enforce_invertibility": False,
                "trend": None,
                "maxiter": int(BEST_MAXITER),
                "tol": 1e-6,
                "disp": False,
                "fallback": "naive_last",
                "seed": 1337,
            }
        },
        f,
    )
print(f"âœ… Saved best config to: {weights_path}")

# 5) è¼¸å‡ºæ­£å¼æäº¤æª”ï¼ˆè©•åˆ†ç³»çµ±é æœŸæª”åé€šå¸¸æ˜¯ submission.pklï¼‰
submission_path = SUBM / "submission.pkl"          # â† æ­£å¼ä¸Šå‚³ç”¨
# submission_path = SUBM / "submission_example.pkl" # â† è‹¥ä½ æƒ³ä¿ç•™æˆ example
submission_full.to_pickle(submission_path)
print(f"âœ… Saved FULL submission to: {submission_path} | rows={len(submission_full)} | windows={submission_full['window_id'].nunique()}")

# 6) é¡¯ç¤ºå‰å¹¾åˆ—ç¢ºèª
display(submission_full.head(12))
